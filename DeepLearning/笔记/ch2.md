一般保证：验证集和test的分布相同



# 方差和偏差

可以理解为：

欠拟合：高偏差

过拟合：高方差



高偏差：在train上表现差

高方差：在train好，在验证数据上表现差



# 正则化

![image-20221223100150305](https://typora-nigel.oss-cn-nanjing.aliyuncs.com/img/image-20221223100150305.png)

L2：是根据模长的平方进行惩罚

L1：对模长进行惩罚

L1正则化会使得W变稀疏（即存在很多0）



在神经网络中，需要对每一层的W都分别进行正则化，



# Dropout

每次正向传播和反向传播的时候，都随机丢掉一些值不参与更新（之后会恢复的）

但在预测阶段，不用Dropout

根据每一层的参数数量大小，设定不同的dropout值，小的就不drop了



## 技术

反向随机Dropout





P53