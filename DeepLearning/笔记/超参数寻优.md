# 超参数寻优

更推荐random search或者Coarse to fine，Grid Search不太推荐

Coarse to fine：通过不断的缩小搜索范围，增快搜索速度



## Log转换搜索空间

例子：搜索范围在0.0001~1之间，转换为变成很多个均匀的区间上搜索。

变成在0.0001~0.001；0.001 ~ 0.01；0.01 ~ 0.1 ；0.1 ~ 1上进行搜索

实现方法：

```
r=np.random.rand()//r在[0,1]
l=10^(-4*r)//因为最小为0.0001，10^-4
```

 

如果是0.99~0.999也可以一样实现

```
r=np.random.rand()
l=1-10^(-3*r)
```





# Batch norm

对隐藏层的神经元进行归一化

注意：$Z^{[l](i)}$ 表示第i个样本在第l层（针对的是第l层的单个神经元），也就是算的mean是按照行算的
$$
\mu =mean(Z^{[l](i)})\\
\sigma^{2}=\frac{1}{m}\sum_{i}(Z^{[l](i)}-\mu)^{2}\\
Z^{[l](i)}_{norm}=\frac{Z^{[l](i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}\\
\tilde{Z}^{[l](i)}=\gamma *Z^{[l](i)}_{norm}+\beta
$$
$\gamma$和$\beta$是类似于参数一样的，可以进行学习的



在Batch norm中，nn模型的参数b是可以忽略的，因为会在归一化的时候就去掉了





# Batch norm 和 mini-Batch

==两者是可以同时使用的==

## 在Test上

可以观察，每次Batch norm算的mean和方差是对所有训练样本算的

但在Test上，往往每次都是单个样本进行预测，那自然Batch norm时，mean和方差就是本来的值，完全没有用

所以为了避免这样的情况，需要通过在Train上的结果，计算mean和方差，以供在Test上使用

常用方法：

1、在Train上的时候，每个mini-Batch都可以在每一层上算出一个mean和方差。如果有n个mini-Batch，那每一层总共就有n个mean和方差

2、通过对这n个mean和方差进行EMA，计算新的mean和方差。这个EMA后的结果，就是Test上使用的



当然，除了EMA还可以有很多方法，通过在Train上的结果计算mean和方差用在Test上