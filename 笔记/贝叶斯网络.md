# 整体架构

https://www.bayesfusion.com/bayesian-networks/

是一个有向无环图的结构

满足一个先后关系，例如：如果一个样本传进来，先在Difficulty和Intelligence中找到所属类别（这两个类别在图中没有父节点，其中的概率值为先验概率），则可以进一步在Grade中得到相应的概率

![image-20221031195019669](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195019669.png)

预测概率的本质公式如下：

![image-20221031195113838](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195113838.png)



分解的好处：

![image-20221031195150866](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195150866.png)

为什么是15个参数？

这个参数表示的是每个情况下的概率

例如：Difficulty，只有1行，两类：也就说只需要一个参数。例如，假设其中一类的概率是0.7，另一类自然就是0.3

（PS：这里这个概率是直接当作参数设定的，与logistics不一样，这是因为这里的数据就是一个1/0特征

再如Grade，有4行，每行预测三类，所以每行的预测函数需要两个参数，所以一共是4*2=8个参数

![image-20221031195202267](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195202267.png)  



例子：

![image-20221031195220049](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195220049.png)

但是要想满足联合概率（即一次性塞进所有变量）等于局部条件概率的乘积，需要满足前提条件：条件独立性。所以我们训练出来的图的结点之间，根据图的结构是要满足条件独立性结构的

![image-20221031195257096](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195257096.png)

回顾后验参数独立性的性质，即只要观察到完整的情况，参数就保持独立。论文24

## 如何确定图的结构

首先一个结点指向另一个结点肯定要满足：父节点影响孩子结点

同时这种关系还必须有传递性



![image-20221031195312393](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195312393.png)

那何时满足这种传递性呢？这取决于结点间的指向情况和其中谁是观测变量

例如：Z表示是否为观测变量，勾还是叉表示是否具有流动性

![image-20221031195345016](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195345016.png)

相关定义

![image-20221031195404393](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195404393.png)

![image-20221031195423317](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195423317.png)

当父节点已知时，该结点与其本身所有非后代结点条件独立



因此，在生成贝叶斯网络中，最后的结果应该是都要满足变量间的流动性的，没有流动性的变量之间的路径会被去掉？



条件独立的使用，推导过程

![image-20221031195435847](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221031195435847.png)





# 训练参数

这里的参数指的是每个类的预测值

例如上图中，假设里面的预测值为$\theta_{1},\theta_{2}\cdots\theta_{k}$ （这里不详细对应到图里了，反正就是一个$\theta$ 对应里面概率表的一个格子）

将这些参数带进去联合概率的分解那个式子（就是上面图片里面的那个），即目标函数，利用最大对数似然求解 

在假设我们观察到的数据是完全的情况下，根据样本的分布进行计算，运用MLE求得最优解（对数似然）,有闭式解

pa表示其父节点，#D表示数量（实际上是算这个概率时的最高次数）

![image-20221105175103493](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221105175103493.png)

但如果数据存在有：

1、隐藏变量

2、缺失值

我们不能是不能得到所有的数据对，所以是没有闭式解的



这样对加大求解参数的难度，因为这种情况下，就算是两层的结构，X1，X2，每选取一个参数，X1的概率值会变，X2也会变，这样就很多种可能性了。（明天再写详细的），这时候的似然函数是一个多峰的情况，也就是没有闭式解

但反观，如果是有闭式解的情况，即数据是完全的，则X1是不会变的了，也就是只需要求X2的参数就行，简单了很多



从最简单的开始，假设我们现在有这么多样本，只有一个变量，应该怎么样才能设定这个概率，才能最好的描述出我们的这个样本呢？如果我们找到了这个参数，那根据这个参数，我们计算的这批样本出现的可能性就是最大的，即最符合这批数据的分布的。

我们的假设是这些样本是符合一个概率分布的，这个概率分布由参数决定，这个参数就是类A的概率。

那我们看一下手头上有的：样本，要求的：参数。根据似然函数，将参数作为未知的，我们只要使得这个似然函数最大，自然的就是这个参数下，这批样本出现的概率最大。而这个似然函数如下。表示对于这批样本，这样出现的概率，（每个样本间的情况相互独立，所以是乘）

这是最简单的情况，如果是多个变量呢？

本质上是：先联合概率计算每个样本出现的概率，再所有样本相乘，即这批样本的联合概率

我们可以根据条件独立的条件对联合概率进行分解，再利用似然函数求里面的所有参数，即概率。式子如下

用MLE有闭式解

这样我们就能够得到最能够表示这些样本的参数了。





论文里面是第一种有闭式解的情况

https://www.youtube.com/watch?v=JZAESnbtKS4





# 训练结构

https://www.youtube.com/watch?v=8N0HsrBY7WI

训练结构其中就包括了训练参数，根据理解，先训练出模型的结构（因为根据BDeu，训练结构的时候不用考虑参数的），再训练参数

避免过拟合：1、设置最大宽度2、设置一个分数函数，惩罚复杂模型，这个被称为soft,sparsity constraints

如果学习的结构不是一棵树，即结点能有多个父结点的时候，会变成np-hard问题



基于分数的训练方法

分数=似然函数值+惩罚项

惩罚项=两部分组成，一部分由数据量计算的值，另一部分是图的参数个数，两者相乘

由数据量计算的值有两种，1、AIC、BIC；2、常数值



分数有：

1、MDL

2、BD、BDe、BDeu

找到一个模型最简单，但偏差小

文中选用的分数为：Bayesian Dirichlet equivalent uniform （BDeu)

可以看到，BDeu分数只和一个超参数有关，但这基于的是数据是完全的情况下，参数独立而且最大似然有闭式解

T是数据，B是网络结构，P(B)网络的先验概率（Heckerman等人将其设置为BS和初始（先验）网络之间的分歧弧数的指数递减函数。）

论文：22

P(BS) is the prior probability of the structure, which Heckerman et al. set to an exponentially decreasing function of the number of differing arcs between BS and the initial (prior) network.

具体的方法可见论文24 Session6

![image-20221103201850852](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221103201850852.png)

等效样本量N`表示了我们对先验分布的信念。BDeu可以观察到，只和那个超参数和一些关于结点的数量有关

![image-20221103201757456](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221103201757456.png)



在训练结构之前，可以根据使用零阶和一阶相关性测试构建变量的无向图或者骨架，用于限制搜索空间（论文中的Figure2）（原理基于参考文献39创建的）



## hill-climbing strategy

一种搜索算法

对于现在的结构，找到现在这个图的neighbor，选择能够使得分数增加的图，依次下去，直到不能增加

但这个解是局部最优解，所以可以通过restarts和randomness来尽力找到全局最优解

这个策略的效率影响因素：

1、需要考虑的图的neighbor数量

2、快速评估neighbor图的能力

3、初始状态的选择



通过考虑的neighbor图，是现在图的添加边、删除边、反转边（注意避免形成环），这个选择是$O(N^{2})$ ，N为变量数量

通常从empty network开始考虑，但也有random starting points or perturbed local optima

Hill-climbing starts with an initial network, which can be empty, random, or constructed from expert knowledge.



因为是基于分数的策略，而分数的计算有着一个很重要的性质：可分解性。一个图的分数可以被视为里面的结点的分数的线性组合。只需要计算neighbor图改变的部分带来的分数变化，就可以快速的根据现有图，计算其neighbor图的分数。

选择分数增加最多的



待完成：

1、、完整阅读里面的参考文献

2、模型的结果是怎么样的



![image-20221105202837627](C:\Users\nigel\AppData\Roaming\Typora\typora-user-images\image-20221105202837627.png)