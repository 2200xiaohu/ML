# 词向量

构建词向量的方法有：

规定：已有词典D，大小为N（词典中记录的是数据中不重复的词）；

1、one-hot representation：用一个很长的向量，大小为N，表示词W：[1,0,0,0,0,...]，1的位置表示为词W在词典中出现的位置

2、Distributed Representation：通过训练，将每一个词都映射到一个固定长度的向量中。方法有：LSA、LDA、神经网络等等





文本分类在 fasttext 中的工作方式是，默认情况下首先使用 skipgram 来表示单词。然后，使用从 skipgram 模型中学习到的这些词向量对输入文本进行分类。您询问的两个参数 (`ws`和`wordNgrams`) 与 skipgram/cbow 模型有关。







# 资料

https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/

https://fasttext.cc/docs/en/supervised-tutorial.html

知乎



video

https://www.youtube.com/watch?v=jWyCo5inkkE

https://www.youtube.com/watch?v=fptTLo8JZDg

https://www.youtube.com/watch?v=UhyelCB5MuA



word2vec

https://www.cnblogs.com/peghoty/p/3857839.html









代码

https://blog.csdn.net/weixin_45707277/article/details/122794848?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166453914016800184113361%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166453914016800184113361&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-122794848-null-null.142^v51^control,201^v3^control_1&utm_term=fasttext&spm=1018.2226.3001.4187
